{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iPgnItHxMtYs"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install numpy accelerate smalldiffusion tqdm diffusers transformers xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms.v2.functional as TF\n",
        "from accelerate import Accelerator\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from torch import nn\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from smalldiffusion import ModelMixin, ScheduleLDM\n",
        "from collections import namedtuple\n",
        "from itertools import pairwise\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N_yds-dNOJ6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this exercise you will implement a custom sampler for a pretrained text-to-image diffusion model (Stable Diffusion 2.1). The goal is to modify classifier-free guidance during the sampling process to:\n",
        "\n",
        "  1. Generate a series of images that interpolates between two different text prompts\n",
        "  2. Generate visual illusions -- images that match different text prompts when viewed from different orientations\n",
        "\n",
        "Stable Diffusion is a [latent diffusion model](https://arxiv.org/abs/2112.10752), where the diffusion process (with a 2D-Unet denoiser model) occurs in a (4x64x64)-dimensional latent space, guided by a text embedding from a pretrained [CLIP model](https://arxiv.org/abs/2103.00020) and tokenizer. The diffusion output in latent-space is decoded into a (3x512x512)-dimeisional pixel-space image with a pretrained variational autoencoder (VAE).\n",
        "\n",
        "First we define a wrapper class for all of these models, which will handle the text encoding and VAE decoding after the diffusion process. We also load the model from a checkpoint."
      ],
      "metadata": {
        "id": "kg7MefLYf5Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alpha_bar(sigma):\n",
        "    return 1/(sigma**2+1)\n",
        "\n",
        "def show_tensor(x):\n",
        "    display(TF.to_pil_image(x))\n",
        "\n",
        "class ModelLatentDiffusion(nn.Module, ModelMixin):\n",
        "    def __init__(self, model_key, accelerator=None):\n",
        "        super().__init__()\n",
        "        self.accelerator = accelerator or Accelerator()\n",
        "        self.vae = AutoencoderKL.from_pretrained(model_key, subfolder=\"vae\")\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(model_key, subfolder=\"tokenizer\")\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(model_key, subfolder=\"text_encoder\")\n",
        "        self.unet = UNet2DConditionModel.from_pretrained(model_key, subfolder=\"unet\")\n",
        "        self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\")\n",
        "        self.input_dims = (self.unet.config.in_channels, self.unet.sample_size, self.unet.sample_size,)\n",
        "        self.text_condition = None\n",
        "        self.text_guidance_scale = None\n",
        "        if is_xformers_available():\n",
        "            self.unet.enable_xformers_memory_efficient_attention()\n",
        "        self.to(self.accelerator.device)\n",
        "\n",
        "    def tokenize(self, prompt):\n",
        "        return self.tokenizer(\n",
        "            prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True, return_tensors='pt'\n",
        "        ).input_ids.to(self.accelerator.device)\n",
        "\n",
        "    def embed_prompt(self, prompt):\n",
        "        with torch.no_grad():\n",
        "            return self.text_encoder(self.tokenize(prompt))[0]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode_latents(self, latents):\n",
        "        decoded = self.vae.decode(latents / 0.18215).sample\n",
        "        normalized = (decoded.squeeze()+1)/2 # from [-1,1] to [0, 1]\n",
        "        return normalized.clamp(0,1)\n",
        "\n",
        "    def sigma_to_t(self, sigma):\n",
        "        idx = torch.searchsorted(reversed(self.scheduler.alphas_cumprod.to(sigma)), alpha_bar(sigma))\n",
        "        return self.scheduler.config.num_train_timesteps - 1 - idx\n",
        "\n",
        "    def forward(self, x, sigma, cond=None):\n",
        "        z = alpha_bar(sigma).sqrt() * x\n",
        "        return self.unet(z, self.sigma_to_t(sigma), encoder_hidden_states=cond).sample\n",
        "\n",
        "model = ModelLatentDiffusion('stabilityai/stable-diffusion-2-1-base')"
      ],
      "metadata": {
        "id": "KD_D_DNZNmSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling with classifier-free guidance\n",
        "\n",
        "Next, we provide a starter sampler code that implements classifier-free guidance. Given $e_0$ (embedding of the empty text prompt, or a negative prompt) and $e_1$ (embedding of the target text prompt) and a guidance scale $\\gamma$, classifier-free guidance is implemented as:\n",
        "\n",
        "$$\\bar{\\epsilon}_t = (1-\\gamma) \\epsilon(x_t, \\sigma_t, e_0) + \\gamma \\epsilon(x_t, \\sigma_t, e_1)$$"
      ],
      "metadata": {
        "id": "awhtcFbZjxEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_cfg(model,\n",
        "               prompt     = 'An astronaut riding a horse',\n",
        "               neg_prompt = '',\n",
        "               cfg_scale  = 7.5,\n",
        "               N          = 50,\n",
        "               gam        = 1.,\n",
        "               mu         = 0.,\n",
        "               seed       = 0,):\n",
        "    model.eval()\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    schedule = ScheduleLDM(1000)\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    # Embed text prompt and negative prompt\n",
        "    embed, neg_embed = map(model.embed_prompt, (prompt, neg_prompt))\n",
        "\n",
        "    # Create sampling noise schedule\n",
        "    sigmas = schedule.sample_sigmas(N)\n",
        "\n",
        "    # Generate\n",
        "    xt = model.rand_input(1).to(accelerator.device) * sigmas[0]\n",
        "    eps = None\n",
        "\n",
        "    for i, (sig, sig_prev) in enumerate(tqdm(pairwise(sigmas))):\n",
        "        # Model is evaluated on xt twice, conditioned on `prompt` and `neg_prompt` respectively\n",
        "        with torch.no_grad():\n",
        "            eps_pred = model.predict_eps(xt, sig.to(xt), embed)\n",
        "            eps_pred_neg = model.predict_eps(xt, sig.to(xt), neg_embed)\n",
        "\n",
        "        # Do classifier-free guidance\n",
        "        eps_prev, eps = eps, (1-cfg_scale) * eps_pred_neg + cfg_scale * eps_pred\n",
        "\n",
        "        # The rest of sampling, adding noise if mu > 0, acceleration if gam > 1.\n",
        "        eps_av = eps * gam + eps_prev * (1-gam)  if i > 0 else eps\n",
        "        sig_p = (sig_prev/sig**mu)**(1/(1-mu)) # sig_prev == sig**mu sig_p**(1-mu)\n",
        "        eta = (sig_prev**2 - sig_p**2).sqrt()\n",
        "        xt = xt - (sig - sig_p) * eps_av + eta * model.rand_input(xt.shape[0]).to(xt)\n",
        "    return model.decode_latents(xt)\n",
        "\n",
        "img = sample_cfg(model,\n",
        "    prompt = 'A photograph of a futuristic city',\n",
        "    neg_prompt = 'low resolution',\n",
        "    cfg_scale  = 7,\n",
        "    gam=1.5, mu=0., N=20, seed=0,\n",
        ")\n",
        "show_tensor(img)"
      ],
      "metadata": {
        "id": "cq9YeAGlj4eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (3 points): Interpolation\n",
        "\n",
        "We can modify classifier-free guidance to interpolate between two different text prompts $e_1$ and $e_2$. This is done by combining the guidance terms and varying the guidance scales $\\gamma_1$ and $\\gamma_2$:\n",
        "\n",
        "\n",
        "$$\\bar{\\epsilon}_t = (1-\\gamma_1-\\gamma_2) \\epsilon(x_t, \\sigma_t, e_0) + \\gamma_1 \\epsilon(x_t, \\sigma_t, e_1) + \\gamma_2 \\epsilon(x_t, \\sigma_t, e_2)$$\n",
        "\n",
        "Implement a version of `sample_cfg` that takes in multiple prompts and guidance scales and performs classifier-free guidance using the different prompts. Code to call this sampler is provided below. Feel free to play around with the default parameters after you get the sampler working."
      ],
      "metadata": {
        "id": "Zpzg9jfesT8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_multi_prompt(model,\n",
        "                        prompts    = ['An astronaut riding a horse',\n",
        "                                      'low resolution, blurry image, bad composition'],\n",
        "                        cfg_scales = [7.5, -6.5],\n",
        "                        N          = 50,\n",
        "                        gam        = 1.,\n",
        "                        mu         = 0.,\n",
        "                        seed       = 0,):\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "qInh5OObnnGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the `sample_multi_prompt` function above to take in multiple prompts and scales,\n",
        "# enabling interpolation between two text prompts\n",
        "for i, scale in enumerate(np.linspace(0, 4, 10)):\n",
        "    img = sample_multi_prompt(\n",
        "        model, gam=1.7, N=20, seed=7,\n",
        "        prompts= ['low resolution, blurry image, bad composition',\n",
        "                  'House in the woods, oil painting, ghibli inspired, high resolution',\n",
        "                  'Building in a futuristic city, oil painting, ghibli inspired, high resolution',],\n",
        "        cfg_scales =(-7, 2+scale, 6-scale),\n",
        "    )\n",
        "    show_tensor(img)"
      ],
      "metadata": {
        "id": "wuT6FK_FcXJZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 (3 points): Visual Illusions\n",
        "\n",
        "We can also use classifier-free guidance to produce images that look like text prompt $e_1$ after applying transformation $t_1$ (e.g. a rotation), but also looks like text prompt $e_2$ after applying transformation $t_2$. This technique is further explored in the [visual anagrams paper](https://dangeng.github.io/visual_anagrams/); we will implement a basic version of it.\n",
        "\n",
        "\n",
        "The main idea is to apply transform $t_1$ to the $x_t$ before applying classifier-free guidance conditioned on $e_1$, then applying its inverse $t_1^{-1}$ to the output. The same is done with $t_2$ and $e_2$.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\bar{\\epsilon}^1_t &= t_1^{-1}((1-\\gamma) \\epsilon(t_1(x_t), \\sigma_t, e_0) + \\gamma \\epsilon(t_1(x_t), \\sigma_t, e_1)) \\\\\n",
        "\\bar{\\epsilon}^2_t &= t_2^{-1}((1-\\gamma) \\epsilon(t_2(x_t), \\sigma_t, e_0) + \\gamma \\epsilon(t_2(x_t), \\sigma_t, e_2))\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The two outputs are then averaged:\n",
        "\n",
        "$$\\bar{\\epsilon}_t = \\frac{1}{2} (\\bar{\\epsilon}^1_t +\\bar{\\epsilon}^2_t)$$\n",
        "\n",
        "Modify your sampler above to apply different transformations for different text prompts. Code to call this sampler to generate and display a visual illusion is provided below. Feel free to play around with the default parameters after you get the sampler working."
      ],
      "metadata": {
        "id": "Iq0sy6dSw8Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_transforms(model,\n",
        "                      prompts  = ['An astronaut riding a horse',\n",
        "                                  'low resolution, blurry image, bad composition'],\n",
        "                      cfg_scales = [7.5, -6.5],\n",
        "                      transforms = None,\n",
        "                      N          = 50,\n",
        "                      gam        = 1.,\n",
        "                      mu         = 0.,\n",
        "                      seed       = 0,):\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "0wchsXDdsNjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms used for illusions\n",
        "Transform = namedtuple('Transform', ['fwd', 'inv'])\n",
        "id_t = Transform(lambda x:x, lambda x:x)\n",
        "r = lambda r: lambda x: TF.rotate(x, r)\n",
        "rot_180 = Transform(r(180), r(-180))\n",
        "\n",
        "transforms = (id_t, rot_180, id_t, rot_180)\n",
        "\n",
        "# Modify the `sample_transforms` function above to take in multiple transforms and\n",
        "# apply them during classifer-free guidance\n",
        "img = sample_transforms(\n",
        "    model, gam=1.2, mu=0.5, N=50, seed=0,\n",
        "    prompts=('', '', # Unconditional null-text prompts\n",
        "            'A painting of a snowy mountain', 'A painting of a horse'),\n",
        "    transforms = transforms,\n",
        "    cfg_scales = (-3, -3, 3.5, 3.5),\n",
        ")\n",
        "imgs_t = [t.fwd(img) for t in transforms[2:]]\n",
        "show_tensor(make_grid(torch.stack(imgs_t)))"
      ],
      "metadata": {
        "id": "jBUL_tZtOiPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZlgTeUEtt_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}