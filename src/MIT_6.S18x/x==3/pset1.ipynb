{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch accelerate matplotlib numpy tqdm smalldiffusion"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6Zf6397dVaJ_"
      },
      "id": "6Zf6397dVaJ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c41da06-f4b8-43e8-9410-6ff1a8960fd4",
      "metadata": {
        "id": "1c41da06-f4b8-43e8-9410-6ff1a8960fd4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from accelerate import Accelerator\n",
        "from itertools import pairwise\n",
        "from IPython.display import HTML\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import mse_loss\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "from smalldiffusion import (\n",
        "    ScheduleLogLinear, training_loop, samples, Swissroll, TimeInputMLP, Schedule,\n",
        "    ModelMixin, get_sigma_embeds\n",
        ")\n",
        "\n",
        "matplotlib.rcParams['animation.embed_limit'] = 2**128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fdb447-dd44-4643-bc73-e8551e15374d",
      "metadata": {
        "collapsed": false,
        "id": "e9fdb447-dd44-4643-bc73-e8551e15374d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "For this exercise you will build diffusion model on a 2D toy dataset from scratch. We will\n",
        "mostly recreate the implementation in the\n",
        "[`smalldiffusion` library](https://github.com/yuanchenyang/smalldiffusion/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c19b608-1814-4a28-a91b-ded27db51f31",
      "metadata": {
        "collapsed": false,
        "id": "2c19b608-1814-4a28-a91b-ded27db51f31"
      },
      "source": [
        "# 1. Load data\n",
        "First we create and load the 2D spiral dataset with 200 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08cd7f0-5698-4343-bd13-4dcdc9a50ab3",
      "metadata": {
        "id": "c08cd7f0-5698-4343-bd13-4dcdc9a50ab3"
      },
      "outputs": [],
      "source": [
        "dataset = Swissroll(np.pi/2, 5*np.pi, 200)\n",
        "loader  = DataLoader(dataset, batch_size=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40c27db-7ce7-47d9-a003-0aa768ddab3e",
      "metadata": {
        "collapsed": false,
        "id": "e40c27db-7ce7-47d9-a003-0aa768ddab3e"
      },
      "source": [
        "Next we define `plot_batch`, which visualizes samples from this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e9552f-d4f3-4e34-a244-d7aa8fef20f4",
      "metadata": {
        "id": "36e9552f-d4f3-4e34-a244-d7aa8fef20f4"
      },
      "outputs": [],
      "source": [
        "def plot_batch(batch, ax=None, **kwargs):\n",
        "    batch = batch.cpu().numpy()\n",
        "    ax = ax or plt\n",
        "    return ax.scatter(batch[:,0], batch[:,1], marker='.', **kwargs)\n",
        "\n",
        "plot_batch(next(iter(loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43d554c-5d98-4296-a985-835a8847578e",
      "metadata": {
        "collapsed": false,
        "id": "c43d554c-5d98-4296-a985-835a8847578e"
      },
      "source": [
        "# 2. Define Schedule\n",
        "Here we define a simple log-linear noise schedule with 200 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b254f42e-9848-46d1-833d-49add0aaa2ec",
      "metadata": {
        "id": "b254f42e-9848-46d1-833d-49add0aaa2ec"
      },
      "outputs": [],
      "source": [
        "schedule = ScheduleLogLinear(N=200, sigma_min=0.005, sigma_max=10)\n",
        "plt.plot(schedule.sigmas)\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$\\\\sigma_t$')\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d31bf1c-8314-44a7-9228-410c54d15a5c",
      "metadata": {
        "collapsed": false,
        "id": "0d31bf1c-8314-44a7-9228-410c54d15a5c"
      },
      "source": [
        "Since our neural network takes the noise level $\\sigma_t$ as input, we need to\n",
        "encode $\\sigma_t$ to ensure it has bounded norm (otherwise $\\sigma_t$ ranges\n",
        "from very small to very large values, making training the neural network\n",
        "ill-conditioned). A simple encoding scheme is to use $[\\sin(\\log(\\sigma)/2), \\cos(\\log(\\sigma)/2)]$.\n",
        "`get_sigma_embeds` is used later in `TimeInputMLP` to encode $\\sigma$ before\n",
        "passing it into a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1535c9f4-6745-4234-9ba7-198e7f3c5bf7",
      "metadata": {
        "id": "1535c9f4-6745-4234-9ba7-198e7f3c5bf7"
      },
      "outputs": [],
      "source": [
        "sx, sy = get_sigma_embeds(len(schedule), schedule.sigmas).T\n",
        "plt.plot(sx, label='$\\\\sin(\\\\log(\\\\sigma_t)/2)$')\n",
        "plt.plot(sy, label='$\\\\cos(\\\\log(\\\\sigma_t)/2)$')\n",
        "plt.xlabel('$t$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69e2851-7818-4119-9b23-eabe5c176373",
      "metadata": {
        "collapsed": false,
        "id": "d69e2851-7818-4119-9b23-eabe5c176373"
      },
      "source": [
        "# 3. Define Model\n",
        "Next we define a simple diffusion model using a MLP. The 4-dimensional input to this MLP\n",
        "is the (2-dimensional) $\\sigma_t$ encoding concatenated with $x$. The MLP has a 2-dimensional output,\n",
        "the predicted noise $\\epsilon$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81ac359-f03f-443b-9046-42f01549c5d5",
      "metadata": {
        "id": "d81ac359-f03f-443b-9046-42f01549c5d5"
      },
      "outputs": [],
      "source": [
        "model = TimeInputMLP(hidden_dims=(16,128,128,128,128,16))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bbc72f-ca17-4b65-a922-180026862eae",
      "metadata": {
        "collapsed": false,
        "id": "54bbc72f-ca17-4b65-a922-180026862eae"
      },
      "source": [
        "# 4. Train Model\n",
        "\n",
        "Next we write code to train the model, optimizing the loss function\n",
        "\n",
        "$$\\mathcal{L}(\\theta) =\n",
        "\\mathbb{E}[\\Vert\\epsilon_\\theta(x_0 + \\sigma \\epsilon, \\sigma) - \\epsilon\\Vert^2]$$\n",
        "\n",
        "**Question 1 (1 point):** Complete the code in the training loop to implement\n",
        "this loss function. Train for 20000 epochs and plot the loss over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9466f8be-8003-49d7-bc9e-c300fb5ce892",
      "metadata": {
        "id": "9466f8be-8003-49d7-bc9e-c300fb5ce892"
      },
      "outputs": [],
      "source": [
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "def training_loop(loader      : DataLoader,\n",
        "                  model       : torch.nn.Module,\n",
        "                  schedule    : Schedule,\n",
        "                  epochs      : int = 10000,\n",
        "                  lr          : float = 1e-3):\n",
        "    accelerator = Accelerator()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    model, optimizer, loader = accelerator.prepare(model, optimizer, loader)\n",
        "    for _ in tqdm(range(epochs)):\n",
        "        for x0 in loader:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Sample random sigmas with same number of batches as x0\n",
        "            sigma = schedule.sample_batch(x0)\n",
        "            while len(sigma.shape) < len(x0.shape):\n",
        "                sigma = sigma.unsqueeze(-1)\n",
        "\n",
        "            # Sample noise with same shape as x0\n",
        "            eps = torch.randn_like(x0)\n",
        "            loss = # mse_loss(..., ...) ### YOUR CODE HERE ###\n",
        "            yield loss.item()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "model = TimeInputMLP(hidden_dims=(16,128,128,128,128,16))\n",
        "losses = list(training_loop(loader, model, schedule, epochs=20000, lr=1e-3))\n",
        "plt.plot(moving_average(losses, 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee547f9-8c85-4595-91cf-2ebbc37acf47",
      "metadata": {
        "collapsed": false,
        "id": "1ee547f9-8c85-4595-91cf-2ebbc37acf47"
      },
      "source": [
        "# 5. Sample from Model\n",
        "\n",
        "First, we plot the predictions of the trained model $\\epsilon_\\theta$ for\n",
        "different values of $\\sigma$.  The following code plots the direction of\n",
        "$\\epsilon_\\theta(x, \\sigma)$ for a fixed $\\sigma$ and different values of $x$\n",
        "from a grid.\n",
        "\n",
        "**Question 2 (1 point):** Play around with different values of $\\sigma$, describing\n",
        "qualitatively how the direction of the predicted noise $\\epsilon_\\theta(x)$ change\n",
        "with $\\sigma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4919a0ae-9992-4605-9f84-5986fb6df0c7",
      "metadata": {
        "id": "4919a0ae-9992-4605-9f84-5986fb6df0c7"
      },
      "outputs": [],
      "source": [
        "def plot_eps_field(model, sigma, ax=None, plot_width=3, mesh_width=21, color='0.5', scale=1,\n",
        "                   scale_by_sigma=True, **kwargs):\n",
        "    ax = plt.gca() if ax is None else ax\n",
        "    mesh_x = np.linspace(-plot_width/2, plot_width/2, mesh_width)\n",
        "    x0s, x1s = np.meshgrid(mesh_x, mesh_x, indexing=\"ij\")\n",
        "    X = torch.tensor(np.vstack((x0s.flatten(), x1s.flatten())).T, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        Y = model.predict_eps(X, sigma)\n",
        "    scaling = sigma if scale_by_sigma else 1/np.linalg.norm(Y, axis=1)\n",
        "    return ax.quiver(X[:, 0], X[:, 1], -Y[:, 0]*scaling, -Y[:, 1]*scaling,\n",
        "                     angles='xy', scale_units='xy', scale=scale, color=color, **kwargs)\n",
        "\n",
        "plot_batch(next(iter(loader)))\n",
        "plot_eps_field(model, schedule.sigmas[0], plot_width=3, scale_by_sigma=False, scale=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3910d70-1ae8-4fea-9bcf-863c57364bcd",
      "metadata": {
        "collapsed": false,
        "id": "d3910d70-1ae8-4fea-9bcf-863c57364bcd"
      },
      "source": [
        "Next, we start with a deterministic DDIM sampler to sample from the model,\n",
        "using the update step:\n",
        "\n",
        "$$x_{t-1} = x_t - (\\sigma_t - \\sigma_{t-1})\\epsilon_\\theta(x_t, \\sigma_t)$$\n",
        "\n",
        "The following code plots the result of sampling 2000 points using 10 sampling steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c1c3c9-fd64-4e45-9cc2-fa01ac7ed912",
      "metadata": {
        "id": "76c1c3c9-fd64-4e45-9cc2-fa01ac7ed912"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "@torch.no_grad()\n",
        "def samples_ddim(model      : torch.nn.Module,\n",
        "                 sigmas     : torch.FloatTensor, # Iterable with N+1 values for N sampling steps\n",
        "                 xt         : Optional[torch.FloatTensor] = None,\n",
        "                 batchsize  : int = 1):\n",
        "    model.eval()\n",
        "    accelerator = Accelerator()\n",
        "    xt = model.rand_input(batchsize).to(accelerator.device) * sigmas[0] if xt is None else xt\n",
        "    for i, (sig, sig_prev) in enumerate(pairwise(sigmas)):\n",
        "        xt = xt - (sig - sig_prev) * model.predict_eps(xt, sig.to(xt))\n",
        "        yield xt\n",
        "\n",
        "sigmas_10 = schedule.sample_sigmas(10) # Subsample 10 steps from training schedule\n",
        "*xts, x0 = samples_ddim(model, sigmas_10, batchsize=2000)\n",
        "plot_batch(x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the sampling trajectory as an animation."
      ],
      "metadata": {
        "id": "dLClhHGgW6vD"
      },
      "id": "dLClhHGgW6vD"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anim(model, loader, sigmas, xts, start_from=0, width=2, quiver_args=None):\n",
        "    quiver_args = quiver_args or dict(\n",
        "        mesh_width=41, color=str(0.6), scale_by_sigma=False, scale=15,\n",
        "        headwidth=2, headlength=3, width=0.002,\n",
        "    )\n",
        "    xts = np.array([s.cpu().numpy() for s in xts])\n",
        "    sigmas, xts = sigmas[start_from:], xts[start_from:]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    ax.set_xlim(-width/2, width/2)\n",
        "    ax.set_ylim(-width/2, width/2)\n",
        "    plot_batch(next(iter(loader)), s=8, ax=ax)\n",
        "    lines = [ax.plot(*xts[:2,i,:].T, color='red')[0] for i in range(xts.shape[1])]\n",
        "    elems = []\n",
        "    def get_quiver(t=0):\n",
        "        while len(elems) > 0:\n",
        "            elems.pop().remove()\n",
        "        for i, line in enumerate(lines):\n",
        "            x, y = xts[:t+2,i,:].T\n",
        "            line.set_xdata(x)\n",
        "            line.set_ydata(y)\n",
        "        quiver = plot_eps_field(model, sigmas[t], ax=ax, plot_width=width, **quiver_args)\n",
        "        elems.append(quiver)\n",
        "        return (quiver,) + tuple(lines)\n",
        "    return FuncAnimation(fig, get_quiver, frames=len(sigmas), interval=200, blit=True)\n",
        "\n",
        "# Generates two circles each with N/2 points\n",
        "def get_xT(N=20, outer_radii=1.3, inner_radii=0.6):\n",
        "    radii = [(inner_radii if i%2 == 0 else outer_radii) for i in range(N)]\n",
        "    th = torch.linspace(0, 2*np.pi, N)\n",
        "    xT = torch.stack([torch.sin(th), torch.cos(th)]).T * torch.tensor(radii).unsqueeze(1)\n",
        "    return xT\n",
        "\n",
        "sigmas_50 = schedule.sample_sigmas(50)\n",
        "xT = get_xT() * sigmas_50[0]\n",
        "xts = list(samples_ddim(model, sigmas_50, xt=xT))\n",
        "ani = get_anim(model, loader, sigmas_50, xts, start_from=10)\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "hhZ1NH-WXryy"
      },
      "id": "hhZ1NH-WXryy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3 (1 point):** Implement an accelerated deterministic sampler (with parameter $\\gamma > 1$) that uses the following update step:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\bar{\\epsilon_t} &= \\gamma \\epsilon_\\theta(x_t, \\sigma_t) + (1-\\gamma)\\epsilon_\\theta(x_{t+1}, \\sigma_{t+1}) \\\\\n",
        "x_{t-1} &= x_t - (\\sigma_t - \\sigma_{t-1})\\bar{\\epsilon_t}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Similar to above, plot the result of 10-step sampling, as well as the animation of sampling trajectories, for different values of $\\gamma$. Describe qualitatively the difference in final samples as well as sampling trajectories when varying $\\gamma$.\n"
      ],
      "metadata": {
        "id": "YCLwWhFOd9KQ"
      },
      "id": "YCLwWhFOd9KQ"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def samples(model      : torch.nn.Module,\n",
        "            sigmas     : torch.FloatTensor, # Iterable with N+1 values for N sampling steps\n",
        "            xt         : Optional[torch.FloatTensor] = None,\n",
        "            gam        : float = 1.,\n",
        "            batchsize  : int = 1):\n",
        "    model.eval()\n",
        "    accelerator = Accelerator()\n",
        "    xt = model.rand_input(batchsize).to(accelerator.device) * sigmas[0] if xt is None else xt\n",
        "    for i, (sig, sig_prev) in enumerate(pairwise(sigmas)):\n",
        "        ### YOUR CODE HERE ###\n",
        "        yield xt\n",
        "\n",
        "*xts, x0 = samples(model, sigmas_10, batchsize=2000, gam=2.0)\n",
        "plot_batch(x0)"
      ],
      "metadata": {
        "id": "d_F4PNPpXP8c"
      },
      "id": "d_F4PNPpXP8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xts = list(samples(model, sigmas_50, xt=xT, gam=2.0))\n",
        "ani = get_anim(model, loader, sigmas_50, xts, start_from=10)\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "ZOpf6KzdlPJM"
      },
      "id": "ZOpf6KzdlPJM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Optional questions (0 points)\n",
        "\n",
        "For students who wish to learn more about diffusion models, here are some optional questions designed to further your understanding of diffusion models.\n",
        "\n",
        "\n",
        "## 6.1 Generalization\n",
        "\n",
        "The swissroll dataset consist of 200 discrete datapoints. A diffusion model trained on these points generalize in the sense that the samples converge to the underlying spiral manifold, not to the discrete points used in training.\n",
        "\n",
        "Reduce the number of points in the spiral (i.e. change `dataset = Swissroll(np.pi/2, 5*np.pi, 200)`) and determine when the diffusion model fail to generalize to the spiral manifold. Play with the trainin/noise schedule parameters and determine which parameters enable better generalization.\n",
        "   \n",
        "## 6.2 Experimenting with samplers\n",
        "Implement DDPM sampling, where noise is added between each diffusion step. Figure out how to vary the noise level to interpolate between DDPM and DDIM. Plot sampling trajectories showing the effect of added noise.\n",
        "\n",
        "## 6.3 Flow matching\n",
        "Implement flow matching training and sampling, where $t$ ranges from 0 to 1 and the training loss is given by:\n",
        "\n",
        "$$\\mathcal{L}(\\theta) =\n",
        "\\mathbb{E}[\\Vert v_\\theta((1-t) x_0 + t \\epsilon, t) - (\\epsilon-x_0)\\Vert^2]$$\n",
        "\n",
        "For sampling, use the following update step for $0 \\le t' < t \\le 1$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "u_1 &\\sim N(0, I) \\\\\n",
        "u_{t'} &= u_t + (t'-t)v_\\theta(u_t, t)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## 6.4 Larger datasets\n",
        "Practice training diffusion models on datasets of different sizes and modalities. To get started, follow the [examples](https://github.com/yuanchenyang/smalldiffusion/tree/main/examples) in the `smalldiffusion` library.\n",
        "\n"
      ],
      "metadata": {
        "id": "8httx3f7hw0D"
      },
      "id": "8httx3f7hw0D"
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3 (ipykernel)",
      "env": null,
      "interrupt_mode": "signal",
      "language": "python",
      "metadata": {
        "debugger": true
      },
      "name": "python3"
    },
    "name": "plot_flow_anim.ipynb",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}